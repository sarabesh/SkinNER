{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8266012,"sourceType":"datasetVersion","datasetId":4906997},{"sourceId":8284125,"sourceType":"datasetVersion","datasetId":4920241},{"sourceId":8296902,"sourceType":"datasetVersion","datasetId":4928570}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install seqeval datasets","metadata":{"execution":{"iopub.status.busy":"2024-05-03T04:42:24.747106Z","iopub.execute_input":"2024-05-03T04:42:24.747526Z","iopub.status.idle":"2024-05-03T04:42:41.506523Z","shell.execute_reply.started":"2024-05-03T04:42:24.747494Z","shell.execute_reply":"2024-05-03T04:42:41.505430Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\nRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=9c9300244e3d0052b415c188ee438dece6fd8fa7f7b762e38dca18a94becf626\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import Dataset\nimport re\nfrom datasets import load_dataset\nimport numpy as np\nimport datasets\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-05-03T04:42:41.508826Z","iopub.execute_input":"2024-05-03T04:42:41.509127Z","iopub.status.idle":"2024-05-03T04:42:46.352110Z","shell.execute_reply.started":"2024-05-03T04:42:41.509098Z","shell.execute_reply":"2024-05-03T04:42:46.351124Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Load the pubmed dataset. the below code will convert the dataset into tokens and tags format.","metadata":{}},{"cell_type":"code","source":"data = []\nfile_path = '/kaggle/input/pubmed/output-combined3.txt'\ncount = 0\nwith open(file_path, 'r') as file:\n    for line in file:\n        data.append(eval(line))","metadata":{"execution":{"iopub.status.busy":"2024-05-03T04:42:46.353180Z","iopub.execute_input":"2024-05-03T04:42:46.353643Z","iopub.status.idle":"2024-05-03T04:42:57.408254Z","shell.execute_reply.started":"2024-05-03T04:42:46.353617Z","shell.execute_reply":"2024-05-03T04:42:57.407254Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"The pubmed dataset extracted and it is in format suitable for spacy model. this format is changed into word tokens and each word is annotated as whether the word is disease or not. ","metadata":{}},{"cell_type":"code","source":"def convert_to_dataset(data):\n    ncbi_data = []\n    for text, annotations in data:\n        tokens = re.findall(r'\\S+', text)  # Tokenize the text using whitespace as delimiter\n        ner_tags = [0] * len(tokens)  # Initialize with \"O\" (outside) tags\n        start, end, label = annotations['entities'][0]  # Assuming there's only one entity\n        char_pos = 0\n        for i, token in enumerate(tokens):\n            if char_pos == start:\n                ner_tags[i] = 1 # \"B-Disease\" for beginning of the entity\n            elif char_pos > start and char_pos < end:\n                ner_tags[i] = 1  # \"I-Disease\" for inside of the entity\n            char_pos += len(tokens[i]) + 1  # Update character position for next token\n        example = {\n            \"id\": str(len(ncbi_data)),  # Generate a unique ID for each example\n            \"tokens\": tokens,\n            \"ner_tags\": ner_tags\n        }\n        ncbi_data.append(example)\n    return Dataset.from_dict({\"id\": [example[\"id\"] for example in ncbi_data],\n                              \"tokens\": [example[\"tokens\"] for example in ncbi_data],\n                              \"ner_tags\": [example[\"ner_tags\"] for example in ncbi_data]})\n\npubmed_dataset = convert_to_dataset(data)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T04:42:57.410101Z","iopub.execute_input":"2024-05-03T04:42:57.410402Z","iopub.status.idle":"2024-05-03T04:43:13.588078Z","shell.execute_reply.started":"2024-05-03T04:42:57.410376Z","shell.execute_reply":"2024-05-03T04:43:13.587010Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"due to computing resource contraint, we shuffled the entire dataset and selected first 100000 samples as train dataset and then 50000 as test dataset. base on this BERT model is fine tuned.","metadata":{}},{"cell_type":"code","source":"pubmed_dataset = pubmed_dataset.shuffle()\npubmed_dataset[10000]","metadata":{"execution":{"iopub.status.busy":"2024-05-03T04:43:13.589424Z","iopub.execute_input":"2024-05-03T04:43:13.589813Z","iopub.status.idle":"2024-05-03T04:43:13.788115Z","shell.execute_reply.started":"2024-05-03T04:43:13.589775Z","shell.execute_reply":"2024-05-03T04:43:13.787230Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'id': '88898',\n 'tokens': ['Probiotics',\n  'may',\n  'make',\n  'little',\n  'or',\n  'no',\n  'difference',\n  'in',\n  'QoL',\n  'for',\n  'people',\n  'with',\n  'eczema',\n  'nor',\n  'in',\n  'investigator-rated',\n  'eczema',\n  'severity',\n  'score',\n  '(combined',\n  'with',\n  'participant',\n  'scoring',\n  'for',\n  'eczema',\n  'symptoms',\n  'of',\n  'itch',\n  'and',\n  'sleep',\n  'loss);',\n  'for',\n  'the',\n  'latter,',\n  'the',\n  'observed',\n  'effect',\n  'was',\n  'small',\n  'and',\n  'of',\n  'uncertain',\n  'clinical',\n  'significance.'],\n 'ner_tags': [0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0]}"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import BertTokenizerFast, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer\n\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n\ndef tokenize_and_align_labels(examples, label_all_tokens=True): \n    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True) \n    labels = [] \n    for i, label in enumerate(examples[\"ner_tags\"]): \n        word_ids = tokenized_inputs.word_ids(batch_index=i) \n        # word_ids() => Return a list mapping the tokens\n        # to their actual word in the initial sentence.\n        # It Returns a list indicating the word corresponding to each token. \n        previous_word_idx = None \n        label_ids = []\n        # Special tokens like `` and `<\\s>` are originally mapped to None \n        # We need to set the label to -100 so they are automatically ignored in the loss function.\n        for word_idx in word_ids: \n            if word_idx is None: \n                # set –100 as the label for these special tokens\n                label_ids.append(-100)\n            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n            # the label_all_tokens flag.\n            elif word_idx != previous_word_idx:\n                # if current word_idx is != prev then its the most regular case\n                # and add the corresponding token                 \n                label_ids.append(label[word_idx]) \n            else: \n                # to take care of sub-words which have the same word_idx\n                # set -100 as well for them, but only if label_all_tokens == False\n                label_ids.append(label[word_idx] if label_all_tokens else -100) \n                # mask the subword representations after the first subword\n                 \n            previous_word_idx = word_idx \n        labels.append(label_ids) \n    tokenized_inputs[\"labels\"] = labels \n    return tokenized_inputs \ntrain_data = Dataset.from_dict(pubmed_dataset[:100000])\nvalidation_data = Dataset.from_dict(pubmed_dataset[100000:150000])\ntokenized_datasets = train_data.map(tokenize_and_align_labels, batched=True)\nvalidation_datasets = validation_data.map(tokenize_and_align_labels, batched=True)\ntokenized_datasets[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-03T04:43:13.790271Z","iopub.execute_input":"2024-05-03T04:43:13.790644Z","iopub.status.idle":"2024-05-03T04:44:16.185749Z","shell.execute_reply.started":"2024-05-03T04:43:13.790611Z","shell.execute_reply":"2024-05-03T04:44:16.184794Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"2024-05-03 04:43:17.495645: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-03 04:43:17.495756: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-03 04:43:17.629927: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5282760980644de81ed02e334c181d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52aee99925a646ae88d6c6886aeb2b65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63a53fe956e943b79151b5926016ab1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d492ee1eaf841b483fa6bc4ab076e2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d9a83bd1c4a41f6a86d555a9cd52753"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99107ade63b04d05a9d48707799090bd"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'id': '367466',\n 'tokens': ['Four',\n  'RCTs',\n  'and',\n  'a',\n  'quasiexperimental',\n  'study',\n  'indicate',\n  'that',\n  'some',\n  'interventions',\n  'can',\n  'enhance',\n  'SSE',\n  'activity',\n  'and',\n  'so',\n  'are',\n  'more',\n  'likely',\n  'to',\n  'aid',\n  'early',\n  'detection',\n  'of',\n  'skin',\n  'cancer.'],\n 'ner_tags': [0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  1],\n 'input_ids': [101,\n  2176,\n  22110,\n  3215,\n  1998,\n  1037,\n  17982,\n  10288,\n  4842,\n  14428,\n  15758,\n  2817,\n  5769,\n  2008,\n  2070,\n  19388,\n  2064,\n  11598,\n  7020,\n  2063,\n  4023,\n  1998,\n  2061,\n  2024,\n  2062,\n  3497,\n  2000,\n  4681,\n  2220,\n  10788,\n  1997,\n  3096,\n  4456,\n  1012,\n  102],\n 'token_type_ids': [0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0],\n 'attention_mask': [1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1],\n 'labels': [-100,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  1,\n  1,\n  -100]}"},"metadata":{}}]},{"cell_type":"markdown","source":"The model training by default store the epoch results in wandb, so Wandb project api key is needed. a free account in weights and biases with project created in that account should generate an apikey that need to be given below, during execution.","metadata":{}},{"cell_type":"code","source":"model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=7)\n\nargs = TrainingArguments( \n\"test-ner\",\nevaluation_strategy = \"epoch\", \nlearning_rate=2e-5, \nper_device_train_batch_size=16, \nper_device_eval_batch_size=16, \nnum_train_epochs=1, \nweight_decay=0.01, \n) \n\ndata_collator = DataCollatorForTokenClassification(tokenizer)\nmetric = datasets.load_metric(\"seqeval\") \nlabel_list = ['O','Skin'] \n\ndef compute_metrics(eval_preds): \n    pred_logits, labels = eval_preds \n    \n    pred_logits = np.argmax(pred_logits, axis=2) \n    predictions = [ \n        [label_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100] \n        for prediction, label in zip(pred_logits, labels) \n    ] \n    true_labels = [ \n      [label_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100] \n       for prediction, label in zip(pred_logits, labels) \n   ] \n    results = metric.compute(predictions=predictions, references=true_labels)\n\n    return { \n          \"precision\": results[\"overall_precision\"], \n          \"recall\": results[\"overall_recall\"], \n          \"f1\": results[\"overall_f1\"], \n          \"accuracy\": results[\"overall_accuracy\"], \n  } \n     \n\n    \ntrainer = Trainer( \n   model, \n   args, \n   train_dataset=tokenized_datasets, \n   eval_dataset=validation_datasets, \n   data_collator=data_collator, \n   tokenizer=tokenizer, \n   compute_metrics=compute_metrics \n) \n\ntrainer.train() \n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T04:44:16.186916Z","iopub.execute_input":"2024-05-03T04:44:16.187203Z","iopub.status.idle":"2024-05-03T05:08:05.679816Z","shell.execute_reply.started":"2024-05-03T04:44:16.187178Z","shell.execute_reply":"2024-05-03T05:08:05.678340Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"176216c363524352811d6b05c3c8ac8a"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_34/2411475112.py:14: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n  metric = datasets.load_metric(\"seqeval\")\n/opt/conda/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"831dd42eb0714d0abb48a8f8bcd06ad5"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240503_044638-cq1pp9ic</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/santhakumaranrakesh/huggingface/runs/cq1pp9ic' target=\"_blank\">balmy-durian-7</a></strong> to <a href='https://wandb.ai/santhakumaranrakesh/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/santhakumaranrakesh/huggingface' target=\"_blank\">https://wandb.ai/santhakumaranrakesh/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/santhakumaranrakesh/huggingface/runs/cq1pp9ic' target=\"_blank\">https://wandb.ai/santhakumaranrakesh/huggingface/runs/cq1pp9ic</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6250/6250 21:07, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.009100</td>\n      <td>0.009009</td>\n      <td>0.982219</td>\n      <td>0.985097</td>\n      <td>0.983656</td>\n      <td>0.996615</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Skin seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=6250, training_loss=0.020296326637268068, metrics={'train_runtime': 1417.9167, 'train_samples_per_second': 70.526, 'train_steps_per_second': 4.408, 'total_flos': 4540905982214976.0, 'train_loss': 0.020296326637268068, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/ner_model\")","metadata":{"execution":{"iopub.status.busy":"2024-05-03T05:10:29.510209Z","iopub.execute_input":"2024-05-03T05:10:29.511204Z","iopub.status.idle":"2024-05-03T05:10:31.658029Z","shell.execute_reply.started":"2024-05-03T05:10:29.511168Z","shell.execute_reply":"2024-05-03T05:10:31.656678Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"tokenizer.save_pretrained(\"/kaggle/working/tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2024-05-03T05:10:36.824888Z","iopub.execute_input":"2024-05-03T05:10:36.825286Z","iopub.status.idle":"2024-05-03T05:10:36.853765Z","shell.execute_reply.started":"2024-05-03T05:10:36.825253Z","shell.execute_reply":"2024-05-03T05:10:36.852592Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/tokenizer/tokenizer_config.json',\n '/kaggle/working/tokenizer/special_tokens_map.json',\n '/kaggle/working/tokenizer/vocab.txt',\n '/kaggle/working/tokenizer/added_tokens.json',\n '/kaggle/working/tokenizer/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"!zip -r tokeniser.zip /kaggle/working/tokenizer/\n!zip -r ner_model.zip /kaggle/working/ner_model/","metadata":{"execution":{"iopub.status.busy":"2024-05-03T05:10:47.933644Z","iopub.execute_input":"2024-05-03T05:10:47.934657Z","iopub.status.idle":"2024-05-03T05:11:13.672790Z","shell.execute_reply.started":"2024-05-03T05:10:47.934608Z","shell.execute_reply":"2024-05-03T05:11:13.671554Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  adding: kaggle/working/tokenizer/ (stored 0%)\n  adding: kaggle/working/tokenizer/special_tokens_map.json (deflated 42%)\n  adding: kaggle/working/tokenizer/tokenizer_config.json (deflated 76%)\n  adding: kaggle/working/tokenizer/vocab.txt (deflated 53%)\n  adding: kaggle/working/tokenizer/tokenizer.json (deflated 71%)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  adding: kaggle/working/ner_model/ (stored 0%)\n  adding: kaggle/working/ner_model/config.json (deflated 54%)\n  adding: kaggle/working/ner_model/model.safetensors (deflated 7%)\n","output_type":"stream"}]},{"cell_type":"code","source":"\nid2label = {\n    str(i): label for i,label in enumerate(label_list)\n}\nlabel2id = {\n    label: str(i) for i,label in enumerate(label_list)\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-03T05:25:52.726950Z","iopub.execute_input":"2024-05-03T05:25:52.727662Z","iopub.status.idle":"2024-05-03T05:25:52.734684Z","shell.execute_reply.started":"2024-05-03T05:25:52.727627Z","shell.execute_reply":"2024-05-03T05:25:52.733180Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import json\nconfig = json.load(open(\"ner_model/config.json\"))\nconfig[\"id2label\"] = id2label\nconfig[\"label2id\"] = label2id\njson.dump(config, open(\"/kaggle/working/ner_model/config.json\",\"w\"))","metadata":{"execution":{"iopub.status.busy":"2024-05-03T05:25:56.567221Z","iopub.execute_input":"2024-05-03T05:25:56.567709Z","iopub.status.idle":"2024-05-03T05:25:56.574841Z","shell.execute_reply.started":"2024-05-03T05:25:56.567679Z","shell.execute_reply":"2024-05-03T05:25:56.573522Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"model_fine_tuned = AutoModelForTokenClassification.from_pretrained(\"/kaggle/working/ner_model\", ignore_mismatched_sizes=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T05:26:00.951328Z","iopub.execute_input":"2024-05-03T05:26:00.952005Z","iopub.status.idle":"2024-05-03T05:26:02.315753Z","shell.execute_reply.started":"2024-05-03T05:26:00.951975Z","shell.execute_reply":"2024-05-03T05:26:02.314541Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at /kaggle/working/ner_model and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([7]) in the checkpoint and torch.Size([2]) in the model instantiated\n- classifier.weight: found shape torch.Size([7, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"\nfrom transformers import pipeline\n\nnlp = pipeline(\"ner\", model=model_fine_tuned, tokenizer=tokenizer)\n\n\nexample = \"Eczema (Atopic Dermatitis): Eczema is a common inflammatory skin condition that causes itching, redness, and rash. It often occurs in individuals with a family history of allergies or asthma and requires careful management to prevent flare-ups and maintain skin health\"\nner_results = nlp(example)\nner_results","metadata":{"execution":{"iopub.status.busy":"2024-05-03T05:28:37.927825Z","iopub.execute_input":"2024-05-03T05:28:37.928170Z","iopub.status.idle":"2024-05-03T05:28:38.046031Z","shell.execute_reply.started":"2024-05-03T05:28:37.928145Z","shell.execute_reply":"2024-05-03T05:28:38.044856Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}]}]}